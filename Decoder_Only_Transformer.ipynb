{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])  \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
    "from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially, a slightly less stochastic version of stochastic gradient descent.\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating Datasets and it's Labels</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''  \n",
    "- Q: \"Who is the King of Cricket?\" → A: \"ViratKohli\"\n",
    "- Q: \"Who is the BigShow of Cricket?\" → A: \"MaxWell\"\n",
    "- Q: \"Who is the best test Captain in India?\" → A: \"ViratKohli\"\n",
    "- Q: \"Who is the Universe Boss?\" → A: \"Chris Gayle\"\n",
    "- Q: \"Who is the Alien in the Cricket Field?\" → A: \"ABdeVilliers\"\n",
    "- Q: \"Who is the 360 degree player in the Cricket ?\" → A: \"ABdeVilliers\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating the vocabulary and mapping that to an token id'''\n",
    "''' This is done because the nn.Embedding() takes tokein id as the input'''\n",
    "\n",
    "token_to_id = {\n",
    "    'who': 0,\n",
    "    'is': 1,\n",
    "    'the': 2,\n",
    "    'king': 3,\n",
    "    'of': 4,\n",
    "    'cricket': 5,\n",
    "    'bigshow': 6,\n",
    "    'best': 7,\n",
    "    'test': 8,\n",
    "    'captain': 9,\n",
    "    'in': 10,\n",
    "    'india': 11,\n",
    "    'universe': 12,\n",
    "    'boss': 13,\n",
    "    'alien': 14,\n",
    "    'field': 15,\n",
    "    '360': 16,\n",
    "    'degree': 17,\n",
    "    'player': 18,\n",
    "    'viratkohli': 19,\n",
    "    'maxwell': 20,\n",
    "    'chris': 21,\n",
    "    'gayle': 22,\n",
    "    'abdevilliers': 23,\n",
    "    '<EOS>': 24, # End of sequence token\n",
    "    '<PAD>': 25 # Padding token\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make the id as the key and token as the value '''\n",
    "\n",
    "id_to_token = dict(map(reversed,token_to_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'who',\n",
       " 1: 'is',\n",
       " 2: 'the',\n",
       " 3: 'king',\n",
       " 4: 'of',\n",
       " 5: 'cricket',\n",
       " 6: 'bigshow',\n",
       " 7: 'best',\n",
       " 8: 'test',\n",
       " 9: 'captain',\n",
       " 10: 'in',\n",
       " 11: 'india',\n",
       " 12: 'universe',\n",
       " 13: 'boss',\n",
       " 14: 'alien',\n",
       " 15: 'field',\n",
       " 16: '360',\n",
       " 17: 'degree',\n",
       " 18: 'player',\n",
       " 19: 'viratkohli',\n",
       " 20: 'maxwell',\n",
       " 21: 'chris',\n",
       " 22: 'gayle',\n",
       " 23: 'abdevilliers',\n",
       " 24: '<EOS>',\n",
       " 25: '<PAD>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define padding token\n",
    "PAD_TOKEN = token_to_id[\"<PAD>\"]  # or you can add it to your token_to_id dictionary\n",
    "\n",
    "  # Create input sequences with padding\n",
    "inputs = torch.tensor([\n",
    "      # Q1: Who is the King of Cricket? (pad to length 11)\n",
    "      [token_to_id[\"who\"], \n",
    "     token_to_id[\"is\"], \n",
    "     token_to_id[\"the\"], \n",
    "     token_to_id[\"king\"], \n",
    "     token_to_id[\"of\"], \n",
    "     token_to_id[\"cricket\"], \n",
    "     token_to_id[\"<EOS>\"], \n",
    "     token_to_id[\"viratkohli\"],\n",
    "     token_to_id[\"<PAD>\"],\n",
    "     token_to_id[\"<PAD>\"],\n",
    "     token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # Q2: Who is the BigShow of Cricket? (pad to length 11)\n",
    "      [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"bigshow\"], \n",
    "     token_to_id[\"of\"], token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"maxwell\"],\n",
    "     token_to_id[\"<PAD>\"],token_to_id[\"<PAD>\"],token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # Q3: Who is the best test Captain in India? (pad to length 11)\n",
    "      [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"best\"], \n",
    "     token_to_id[\"test\"], token_to_id[\"captain\"], token_to_id[\"in\"], token_to_id[\"india\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"viratkohli\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # Q4: Who is the Universe Boss? (pad to length 11)\n",
    "      [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"universe\"], \n",
    "     token_to_id[\"boss\"], token_to_id[\"<EOS>\"], token_to_id[\"chris\"], token_to_id[\"gayle\"],\n",
    "     token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # Q5: Who is the Alien in the Cricket Field? (pad to length 11)\n",
    "      [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"alien\"], \n",
    "     token_to_id[\"in\"], token_to_id[\"the\"], token_to_id[\"cricket\"], token_to_id[\"field\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # Q6: Who is the 360 degree player in the Cricket? (already length 11)\n",
    "      [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"360\"], \n",
    "     token_to_id[\"degree\"], token_to_id[\"player\"], token_to_id[\"in\"], token_to_id[\"the\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"]]\n",
    "  ])\n",
    "\n",
    "\n",
    "labels = torch.tensor([\n",
    "      # A1: ViratKohli\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"king\"], token_to_id[\"of\"], \n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"viratkohli\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # A2: Maxwell\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"bigshow\"], token_to_id[\"of\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"maxwell\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # A3: ViratKohli\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"best\"], token_to_id[\"test\"],\n",
    "     token_to_id[\"captain\"], token_to_id[\"in\"], token_to_id[\"india\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"viratkohli\"], token_to_id[\"<EOS>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # A4: Chris Gayle\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"universe\"], token_to_id[\"boss\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"chris\"], token_to_id[\"gayle\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # A5: ABdeVilliers\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"alien\"], token_to_id[\"in\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"field\"], token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"<PAD>\"], token_to_id[\"<PAD>\"]],\n",
    "    \n",
    "      # A6: ABdeVilliers\n",
    "      [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"360\"], token_to_id[\"degree\"],\n",
    "     token_to_id[\"player\"], token_to_id[\"in\"], token_to_id[\"cricket\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"abdevilliers\"], token_to_id[\"<EOS>\"], token_to_id[\"<PAD>\"]]\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs,labels)\n",
    "dataloader = DataLoader(dataset) # if we are going to handle the large number of datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positional Encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn.Module in PyTorch is a powerful base class that provides the fundamental building blocks for creating neural networks.It also provides Built-in Parameter management'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Positional Encoding to keep track of the order of the tokens/words\n",
    "\n",
    "We use sine and cosine functions for positional encoding, where the number of dimensions matches our embedding dimension.\n",
    "For each position, we generate a unique pattern using these trigonometric functions.\n",
    "\n",
    "For a d_model dimensional embedding vector at position pos:\n",
    "- Even indices (2i): PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- Odd indices (2i+1): PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "This creates a unique encoding for each position that:\n",
    "1. Is deterministic and requires no training\n",
    "2. Can handle variable sequence lengths\n",
    "3. Has consistent relative distances between positions\n",
    "4. Allows the model to easily attend to relative positions'''\n",
    "\n",
    "'''nn.Module in PyTorch is a powerful base class that provides the fundamental building blocks for creating neural networks.It also provides Built-in Parameter management'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(start=0, end=11, step=1).float().unsqueeze(0)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model = 2 , max_len=11):\n",
    "\n",
    "        '''\n",
    "        Since we are using 2-dimensional word embeddings (using sine and cosine waves) to represent each word/token, the d_model (model dimension) will be 2. This means each token in our transformer will be represented by a 2-dimensional vector in the embedding space. This d_model value will remain consistent throughout all layers of the transformer architecture, including attention mechanisms and feed-forward networks.\n",
    "        max_len refers to the maximum length of the sentence , the maximum length of the sentence in our vocabulary is 11.\n",
    "        '''\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.pe = torch.zeros(1, max_len, d_model)  # 3D tensor: [1, max_len, d_model]\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Shape: [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        self.pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe  # Register as a buffer if using `nn.Module`\n",
    "        ''' \n",
    "        pe =[[sin(0), cos(0), sin(0), cos(0)],\n",
    "            [sin(1), cos(1), sin(1/10000^(2/d_model)), cos(1/10000^(2/d_model))],\n",
    "            [sin(2), cos(2), sin(2/10000^(2/d_model)), cos(2/10000^(2/d_model))],\n",
    "            [sin(3), cos(3), sin(3/10000^(2/d_model)), cos(3/10000^(2/d_model))],\n",
    "            [sin(4), cos(4), sin(4/10000^(2/d_model)), cos(4/10000^(2/d_model))],\n",
    "            [sin(5), cos(5), sin(5/10000^(2/d_model)), cos(5/10000^(2/d_model))],\n",
    "            [sin(6), cos(6), sin(6/10000^(2/d_model)), cos(6/10000^(2/d_model))],\n",
    "            [sin(7), cos(7), sin(7/10000^(2/d_model)), cos(7/10000^(2/d_model))],\n",
    "            [sin(8), cos(8), sin(8/10000^(2/d_model)), cos(8/10000^(2/d_model))],\n",
    "            [sin(9), cos(9), sin(9/10000^(2/d_model)), cos(9/10000^(2/d_model))],\n",
    "            [sin(10), cos(10), sin(10/10000^(2/d_model)), cos(10/10000^(2/d_model))]]\n",
    "        '''\n",
    "\n",
    "    '''Addition of word embeddings with the positional encoding'''\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "        seq_len = word_embeddings.size(1)  \n",
    "        # Get the sequence length from word_embeddings\n",
    "        # Adjust pe to match the batch size and sequence length of word_embeddings\n",
    "        return word_embeddings + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' We are going to create the self attention and the masked self attention score'''\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model = 2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #  Creating the query , key ,values for the word emebeddings \n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0 \n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self,encoding_for_q,encoding_for_k,encoding_for_v,mask=None):\n",
    "\n",
    "        q = self.W_q(encoding_for_q)\n",
    "        k = self.W_k(encoding_for_k)\n",
    "        v = self.W_v(encoding_for_v)\n",
    "\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim)) # Creating the query key relationships\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5) # dividing the query key values with the square root of the dimension of the transformer\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim) # Creating the attention scores with the softmax\n",
    "        attention_scores = torch.matmul(attention_percents, v) # Multiplying the attention scores with the values to get the attention scores\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decoder Only Transformer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_tokens, d_model=2, max_len=11, num_heads=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, \n",
    "                             embedding_dim=d_model)     \n",
    "        \n",
    "        self.pe = Positional_Encoding(d_model=d_model, \n",
    "                                 max_len=max_len)\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            Attention(d_model=d_model) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Add layer to reduce concatenated attention outputs back to d_model dimension\n",
    "        self.reduce_attention_dim = nn.Linear(in_features=(num_heads*d_model), \n",
    "                                            out_features=d_model)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)        \n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        \n",
    "        # Create attention mask\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), \n",
    "                                   device=self.device))\n",
    "        mask = mask == 0\n",
    "        \n",
    "        # Calculate attention for each head\n",
    "        attention_outputs = []\n",
    "        for attention_head in self.attention_heads:\n",
    "            attention_output = attention_head(position_encoded,\n",
    "                                           position_encoded,\n",
    "                                           position_encoded,\n",
    "                                           mask=mask)\n",
    "            attention_outputs.append(attention_output)\n",
    "        \n",
    "        # Concatenate all attention outputs\n",
    "        all_attention_values = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # Reduce dimension back to d_model\n",
    "        final_attention_values = self.reduce_attention_dim(all_attention_values)\n",
    "        \n",
    "        # Add residual connection\n",
    "        residual_connection_values = position_encoded + final_attention_values\n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        \n",
    "        return fc_layer_output\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        input_tokens, labels = batch\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderOnlyTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_to_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.__init__\u001b[1;34m(self, num_tokens, d_model, max_len, num_heads)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m=\u001b[39m d_model\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_embeddings\u001b[38;5;241m=\u001b[39mnum_tokens, \n\u001b[0;32m     12\u001b[0m                      embedding_dim\u001b[38;5;241m=\u001b[39md_model)     \n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe \u001b[38;5;241m=\u001b[39m \u001b[43mPositional_Encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create multiple attention heads\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_heads \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m     19\u001b[0m     Attention(d_model\u001b[38;5;241m=\u001b[39md_model) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m     20\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m, in \u001b[0;36mPositional_Encoding.__init__\u001b[1;34m(self, d_model, max_len)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, max_len, d_model)  \u001b[38;5;66;03m# 3D tensor: [1, max_len, d_model]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, max_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [max_len, 1]\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m div_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10000.0\u001b[39m) \u001b[38;5;241m/\u001b[39m d_model))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=11)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5, 24, 19, 25, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor([\n",
    "     token_to_id[\"who\"], \n",
    "     token_to_id[\"is\"], \n",
    "     token_to_id[\"the\"], \n",
    "     token_to_id[\"king\"], \n",
    "     token_to_id[\"of\"], \n",
    "     token_to_id[\"cricket\"], \n",
    "     token_to_id[\"<EOS>\"], \n",
    "     token_to_id[\"viratkohli\"],\n",
    "     token_to_id[\"<PAD>\"],\n",
    "     token_to_id[\"<PAD>\"],\n",
    "     token_to_id[\"<PAD>\"]])\n",
    "print(model_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum token ID in input: 25\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum token ID in input: {model_input.max().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max ID: 25, Min ID: 0\n"
     ]
    }
   ],
   "source": [
    "max_id = model_input.max()\n",
    "min_id = model_input.min()\n",
    "print(f\"Max ID: {max_id}, Min ID: {min_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_length \u001b[38;5;241m=\u001b[39m model_input\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[44], line 31\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids):\n\u001b[0;32m     30\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwe(token_ids)        \n\u001b[1;32m---> 31\u001b[0m     position_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Create attention mask\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), \n\u001b[0;32m     35\u001b[0m                                device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[31], line 47\u001b[0m, in \u001b[0;36mPositional_Encoding.forward\u001b[1;34m(self, word_embeddings)\u001b[0m\n\u001b[0;32m     44\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m word_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Get the sequence length from word_embeddings\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Adjust pe to match the batch size and sequence length of word_embeddings\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m word_embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "input_length = model_input.size(dim=0)\n",
    "predictions = model(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((model_input, predicted_id))\n\u001b[1;32m----> 8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      9\u001b[0m predicted_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])])\n\u001b[0;32m     10\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((predicted_ids, predicted_id))\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[44], line 31\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids):\n\u001b[0;32m     30\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwe(token_ids)        \n\u001b[1;32m---> 31\u001b[0m     position_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Create attention mask\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), \n\u001b[0;32m     35\u001b[0m                                device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mageshwaran18\\anaconda3\\envs\\ollama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[31], line 47\u001b[0m, in \u001b[0;36mPositional_Encoding.forward\u001b[1;34m(self, word_embeddings)\u001b[0m\n\u001b[0;32m     44\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m word_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Get the sequence length from word_embeddings\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Adjust pe to match the batch size and sequence length of word_embeddings\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m word_embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "max_length = 25\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    \n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
