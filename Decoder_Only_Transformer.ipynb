{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])  \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
    "from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially, a slightly less stochastic version of stochastic gradient descent.\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating Datasets and it's Labels</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''  \n",
    "- Q: \"Who is the King of Cricket?\" → A: \"ViratKohli\"\n",
    "- Q: \"Who is the BigShow of Cricket?\" → A: \"MaxWell\"\n",
    "- Q: \"Who is the best test Captain in India?\" → A: \"ViratKohli\"\n",
    "- Q: \"Who is the Universe Boss?\" → A: \"Chris Gayle\"\n",
    "- Q: \"Who is the Alien in the Cricket Field?\" → A: \"ABdeVilliers\"\n",
    "- Q: \"Who is the 360 degree player in the Cricket ?\" → A: \"ABdeVilliers\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating the vocabulary and mapping that to an token id'''\n",
    "''' This is done because the nn.Embedding() takes tokein id as the input'''\n",
    "\n",
    "token_to_id = {\n",
    "    'who': 0,\n",
    "    'is': 1,\n",
    "    'the': 2,\n",
    "    'king': 3,\n",
    "    'of': 4,\n",
    "    'cricket': 5,\n",
    "    'bigshow': 6,\n",
    "    'best': 7,\n",
    "    'test': 8,\n",
    "    'captain': 9,\n",
    "    'in': 10,\n",
    "    'india': 11,\n",
    "    'universe': 12,\n",
    "    'boss': 13,\n",
    "    'alien': 14,\n",
    "    'field': 15,\n",
    "    '360': 16,\n",
    "    'degree': 17,\n",
    "    'player': 18,\n",
    "    'viratkohli': 19,\n",
    "    'maxwell': 20,\n",
    "    'chris': 21,\n",
    "    'gayle': 22,\n",
    "    'abdevilliers': 23,\n",
    "    '<EOS>': 24  # End of sequence token\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make the id as the key and token as the value '''\n",
    "\n",
    "id_to_token = dict(map(reversed,token_to_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'who',\n",
       " 1: 'is',\n",
       " 2: 'the',\n",
       " 3: 'king',\n",
       " 4: 'of',\n",
       " 5: 'cricket',\n",
       " 6: 'bigshow',\n",
       " 7: 'best',\n",
       " 8: 'test',\n",
       " 9: 'captain',\n",
       " 10: 'in',\n",
       " 11: 'india',\n",
       " 12: 'universe',\n",
       " 13: 'boss',\n",
       " 14: 'alien',\n",
       " 15: 'field',\n",
       " 16: '360',\n",
       " 17: 'degree',\n",
       " 18: 'player',\n",
       " 19: 'viratkohli',\n",
       " 20: 'maxwell',\n",
       " 21: 'chris',\n",
       " 22: 'gayle',\n",
       " 23: 'abdevilliers',\n",
       " 24: '<EOS>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' We are creating the dataset for the input using the token to id'''\n",
    "\n",
    "# Define padding token\n",
    "PAD_TOKEN = -1  # or you can add it to your token_to_id dictionary\n",
    "\n",
    "# Create input sequences with padding\n",
    "inputs = torch.tensor([\n",
    "    # Q1: Who is the King of Cricket? (pad to length 11)\n",
    "    [token_to_id[\"who\"], \n",
    "     token_to_id[\"is\"], \n",
    "     token_to_id[\"the\"], \n",
    "     token_to_id[\"king\"], \n",
    "     token_to_id[\"of\"], \n",
    "     token_to_id[\"cricket\"], \n",
    "     token_to_id[\"<EOS>\"], \n",
    "     token_to_id[\"viratkohli\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # Q2: Who is the BigShow of Cricket? (pad to length 11)\n",
    "    [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"bigshow\"], \n",
    "     token_to_id[\"of\"], token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"maxwell\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # Q3: Who is the best test Captain in India? (pad to length 11)\n",
    "    [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"best\"], \n",
    "     token_to_id[\"test\"], token_to_id[\"captain\"], token_to_id[\"in\"], token_to_id[\"india\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"viratkohli\"], PAD_TOKEN],\n",
    "    \n",
    "    # Q4: Who is the Universe Boss? (pad to length 11)\n",
    "    [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"universe\"], \n",
    "     token_to_id[\"boss\"], token_to_id[\"<EOS>\"], token_to_id[\"chris\"], token_to_id[\"gayle\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # Q5: Who is the Alien in the Cricket Field? (pad to length 11)\n",
    "    [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"alien\"], \n",
    "     token_to_id[\"in\"], token_to_id[\"the\"], token_to_id[\"cricket\"], token_to_id[\"field\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"], PAD_TOKEN],\n",
    "    \n",
    "    # Q6: Who is the 360 degree player in the Cricket? (already length 11)\n",
    "    [token_to_id[\"who\"], token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"360\"], \n",
    "     token_to_id[\"degree\"], token_to_id[\"player\"], token_to_id[\"in\"], token_to_id[\"the\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"]]\n",
    "])\n",
    "\n",
    "\n",
    "labels = torch.tensor([\n",
    "    # A1: ViratKohli\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"king\"], token_to_id[\"of\"], \n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"viratkohli\"], token_to_id[\"<EOS>\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # A2: Maxwell\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"bigshow\"], token_to_id[\"of\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"<EOS>\"], token_to_id[\"maxwell\"], token_to_id[\"<EOS>\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # A3: ViratKohli\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"best\"], token_to_id[\"test\"],\n",
    "     token_to_id[\"captain\"], token_to_id[\"in\"], token_to_id[\"india\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"viratkohli\"], token_to_id[\"<EOS>\"], PAD_TOKEN],\n",
    "    \n",
    "    # A4: Chris Gayle\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"universe\"], token_to_id[\"boss\"],\n",
    "     token_to_id[\"<EOS>\"], token_to_id[\"chris\"], token_to_id[\"gayle\"], token_to_id[\"<EOS>\"],\n",
    "     PAD_TOKEN, PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # A5: ABdeVilliers\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"alien\"], token_to_id[\"in\"],\n",
    "     token_to_id[\"cricket\"], token_to_id[\"field\"], token_to_id[\"<EOS>\"], token_to_id[\"abdevilliers\"],\n",
    "     token_to_id[\"<EOS>\"], PAD_TOKEN, PAD_TOKEN],\n",
    "    \n",
    "    # A6: ABdeVilliers\n",
    "    [token_to_id[\"is\"], token_to_id[\"the\"], token_to_id[\"360\"], token_to_id[\"degree\"],\n",
    "     token_to_id[\"player\"], token_to_id[\"in\"], token_to_id[\"cricket\"], token_to_id[\"<EOS>\"],\n",
    "     token_to_id[\"abdevilliers\"], token_to_id[\"<EOS>\"], PAD_TOKEN]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs,labels)\n",
    "dataloader = DataLoader(dataset) # if we are going to handle the large number of datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positional Encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn.Module in PyTorch is a powerful base class that provides the fundamental building blocks for creating neural networks.It also provides Built-in Parameter management'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Positional Encoding to keep track of the order of the tokens/words\n",
    "\n",
    "We use sine and cosine functions for positional encoding, where the number of dimensions matches our embedding dimension.\n",
    "For each position, we generate a unique pattern using these trigonometric functions.\n",
    "\n",
    "For a d_model dimensional embedding vector at position pos:\n",
    "- Even indices (2i): PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- Odd indices (2i+1): PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "This creates a unique encoding for each position that:\n",
    "1. Is deterministic and requires no training\n",
    "2. Can handle variable sequence lengths\n",
    "3. Has consistent relative distances between positions\n",
    "4. Allows the model to easily attend to relative positions'''\n",
    "\n",
    "'''nn.Module in PyTorch is a powerful base class that provides the fundamental building blocks for creating neural networks.It also provides Built-in Parameter management'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(start=0, end=11, step=1).float().unsqueeze(0)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model = 2 , max_len=11):\n",
    "\n",
    "        '''\n",
    "        Since we are using 2-dimensional word embeddings (using sine and cosine waves) to represent each word/token, the d_model (model dimension) will be 2. This means each token in our transformer will be represented by a 2-dimensional vector in the embedding space. This d_model value will remain consistent throughout all layers of the transformer architecture, including attention mechanisms and feed-forward networks.\n",
    "        max_len refers to the maximum length of the sentence , the maximum length of the sentence in our vocabulary is 11.\n",
    "        '''\n",
    "        super().__init__() # Intializing the weights for the neural network \n",
    "\n",
    "        pe = torch.zeros(max_len,d_model) # Creating the zeros matric for the positional encoding with repective to the transformer's dimensions and the max_len\n",
    "\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1) # We are creating the positions for the positional encoding , we want float which will help in the training and unsqueeze helps to transform the horizontal list into a vertical list , nested list ex : if not unsqueeze it returns tensor([[0.,1.,2.]]) else tensor([[0.],[1.],[2.]])\n",
    "\n",
    "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        ''' Embedding values will be [0,2] which defines that 0 and 2nd index will the cos and 1 and 3rd index will be the sin  '''\n",
    "\n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "\n",
    "        pe[: , 0::2] = torch.sin(position * div_term)\n",
    "        pe[: , 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        ''' \n",
    "        pe =[[sin(0), cos(0), sin(0), cos(0)],\n",
    "            [sin(1), cos(1), sin(1/10000^(2/d_model)), cos(1/10000^(2/d_model))],\n",
    "            [sin(2), cos(2), sin(2/10000^(2/d_model)), cos(2/10000^(2/d_model))],\n",
    "            [sin(3), cos(3), sin(3/10000^(2/d_model)), cos(3/10000^(2/d_model))],\n",
    "            [sin(4), cos(4), sin(4/10000^(2/d_model)), cos(4/10000^(2/d_model))],\n",
    "            [sin(5), cos(5), sin(5/10000^(2/d_model)), cos(5/10000^(2/d_model))],\n",
    "            [sin(6), cos(6), sin(6/10000^(2/d_model)), cos(6/10000^(2/d_model))],\n",
    "            [sin(7), cos(7), sin(7/10000^(2/d_model)), cos(7/10000^(2/d_model))],\n",
    "            [sin(8), cos(8), sin(8/10000^(2/d_model)), cos(8/10000^(2/d_model))],\n",
    "            [sin(9), cos(9), sin(9/10000^(2/d_model)), cos(9/10000^(2/d_model))],\n",
    "            [sin(10), cos(10), sin(10/10000^(2/d_model)), cos(10/10000^(2/d_model))]]\n",
    "        '''\n",
    "\n",
    "    '''Addition of word embeddings with the positional encoding'''\n",
    "    \n",
    "    def forward(self,word_embeddings):\n",
    "\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), : ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Attention </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' We are going to create the self attention and the masked self attention score'''\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model = 2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #  Creating the query , key ,values for the word emebeddings \n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0 \n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self,encoding_for_q,encoding_for_k,encoding_for_v,mask=None):\n",
    "\n",
    "        q = self.W_q(encoding_for_q)\n",
    "        k = self.W_k(encoding_for_k)\n",
    "        v = self.W_v(encoding_for_v)\n",
    "\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim)) # Creating the query key relationships\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5) # dividing the query key values with the square root of the dimension of the transformer\n",
    "\n",
    "        if mask is not None:\n",
    "\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim) # Creating the attention scores with the softmax\n",
    "        attention_scores = torch.matmul(attention_percents, v) # Multiplying the attention scores with the values to get the attention scores\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decoder Only Transformer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_tokens, d_model=2, max_len=11, num_heads=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, \n",
    "                             embedding_dim=d_model)     \n",
    "        \n",
    "        self.pe = Positional_Encoding(d_model=d_model, \n",
    "                                 max_len=max_len)\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            Attention(d_model=d_model) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Add layer to reduce concatenated attention outputs back to d_model dimension\n",
    "        self.reduce_attention_dim = nn.Linear(in_features=(num_heads*d_model), \n",
    "                                            out_features=d_model)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)        \n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        \n",
    "        # Create attention mask\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), \n",
    "                                   device=self.device))\n",
    "        mask = mask == 0\n",
    "        \n",
    "        # Calculate attention for each head\n",
    "        attention_outputs = []\n",
    "        for attention_head in self.attention_heads:\n",
    "            attention_output = attention_head(position_encoded,\n",
    "                                           position_encoded,\n",
    "                                           position_encoded,\n",
    "                                           mask=mask)\n",
    "            attention_outputs.append(attention_output)\n",
    "        \n",
    "        # Concatenate all attention outputs\n",
    "        all_attention_values = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # Reduce dimension back to d_model\n",
    "        final_attention_values = self.reduce_attention_dim(all_attention_values)\n",
    "        \n",
    "        # Add residual connection\n",
    "        residual_connection_values = position_encoded + final_attention_values\n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        \n",
    "        return fc_layer_output\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        input_tokens, labels = batch\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAi_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
